{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install sentencepiece","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport transformers\nimport torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport nltk\nnltk.download('punkt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Summary","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/content/original_train.csv\")    # original training set from organizers\ntest_df = pd.read_csv(\"/content/original_test.csv\")      # original test set from organizers\ndev_df = pd.read_csv(\"/content/original_dev.csv\")        # original dev set from organizers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_long_text_summary(long_text, max_length_per_section):\n    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to('cuda')\n\n    # Split the text into smaller sections\n    sections = [long_text[i:i + max_length_per_section] for i in range(0, len(long_text), max_length_per_section)]\n\n    summaries = []\n\n    for section in sections:\n        input_text = \"summarize: \" + section\n        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1000, truncation=True, padding=True)\n\n        # Adjust max_length and length_penalty as needed\n        summary_ids = model.generate(inputs.to('cuda'), max_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)\n\n        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n        summaries.append(summary)\n\n    # Concatenate the summaries for each section\n    final_summary = \" \".join(summaries)\n    return final_summary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_double_summary(df):\n    # Iterate through each row and generate summaries\n    for index, row in df.iterrows():\n        try:\n          input_text = row['explanation']\n\n          # Generate the first summary\n          summary = generate_long_text_summary(input_text, 1000)\n\n          # Use the first summary as input for the second summary\n          input_text = summary\n          summary_new = generate_long_text_summary(input_text, 300)\n\n          # Store the final summary in the 'summary' column\n          df.at[index, 'summary'] = summary_new\n        except:\n          df.at[index, 'summary'] = \" \"\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate 2nd level summary on all 3 sets\ndf_train = generate_double_summary(train_df)\ndf_test = generate_double_summary(test_df)\ndf_dev = generate_double_summary(dev_df)\n\n# Save all 3 dataframes\ndf_train.to_csv(\"/content/summary_train.csv\")\ndf_test.to_csv(\"/content/summary_test.csv\")\ndf_dev.to_csv(\"/content/summary_dev.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dev = pd.read_csv(\"/content/summary_dev.csv\") # Summarized Dev set\ndf_train = pd.read_csv(\"/content/summary_train.csv\") # Summarized Train set\ndf_test = pd.read_csv(\"/content/summary_test.csv\") # Summarized Test set","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer embeddings","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('LambdaX-AI/legal-deberta-v1')\ndeberta_model = AutoModel.from_pretrained('LambdaX-AI/legal-deberta-v1').to('cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get sentence embeddings using Transformer\ndef get_embeddings(sentence):\n    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n    inputs = inputs.to('cuda')\n    outputs = deberta_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n    return embeddings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings for questions and answers for training set\ndf_train['question_embeddings'] = df_train['question'].apply(get_embeddings)\ndf_train['answer_embeddings'] = df_train['answer'].apply(get_embeddings)\ndf_train['summary_embeddings'] = df_train['summary'].apply(get_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings for questions and answers for dev set\ndf_dev['question_embeddings'] = df_dev['question'].apply(get_embeddings)\ndf_dev['answer_embeddings'] = df_dev['answer'].apply(get_embeddings)\ndf_dev['summary_embeddings'] = df_dev['summary'].apply(get_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings for questions and answers for test set\ndf_test['question_embeddings'] = df_test['question'].apply(get_embeddings)\ndf_test['answer_embeddings'] = df_test['answer'].apply(get_embeddings)\ndf_test['summary_embeddings'] = df_test['summary'].apply(get_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec embeddings","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\n\n\n# Tokenize the sentences\ndf_train['question_tokens'] = df_train['question'].apply(word_tokenize)\ndf_train['answer_tokens'] = df_train['answer'].apply(word_tokenize)\ndf_train['summary_tokens'] = df_train['summary'].apply(word_tokenize)\n\ndf_dev['question_tokens'] = df_dev['question'].apply(word_tokenize)\ndf_dev['answer_tokens'] = df_dev['answer'].apply(word_tokenize)\ndf_dev['summary_tokens'] = df_dev['summary'].apply(word_tokenize)\n\ndf_test['question_tokens'] = df_test['question'].apply(word_tokenize)\ndf_test['answer_tokens'] = df_test['answer'].apply(word_tokenize)\ndf_test['summary_tokens'] = df_test['summary'].apply(word_tokenize)\n\nmy_embedding_size = 5\n# Train a Word2Vec model\nword2vec_model = Word2Vec(sentences=df_train['question_tokens'].tolist() +\n                                   df_train['answer_tokens'].tolist() +\n                                   df_train['summary_tokens'].tolist() +\n                                   df_dev['question_tokens'].tolist() +\n                                   df_dev['answer_tokens'].tolist() +\n                                   df_dev['summary_tokens'].tolist() +\n                                   df_test['question_tokens'].tolist() +\n                                   df_test['answer_tokens'].tolist() +\n                                   df_test['summary_tokens'].tolist(),\n                          vector_size=my_embedding_size,  # Setting the desired size of embeddings\n                          window=7,  # Adjusting the window size\n                          min_count=1,  # Minimum word frequency to be included in the model\n                          workers=2)  # Number of CPU cores to use during training\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word2vec_embeddings(tokens):\n    embeddings = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n    if not embeddings:\n        # If no word has embeddings, return zeros or handle as needed\n        return [0.0] * my_embedding_size\n    return embeddings\n\n# Get Word2Vec embeddings for questions, answers, and summaries\ndf_train['question_embeddings'] = df_train['question_tokens'].apply(get_word2vec_embeddings)\ndf_train['answer_embeddings'] = df_train['answer_tokens'].apply(get_word2vec_embeddings)\ndf_train['summary_embeddings'] = df_train['summary_tokens'].apply(get_word2vec_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Word2Vec embeddings for questions, answers, and summaries for test and dev set\ndf_test['question_embeddings'] = df_test['question_tokens'].apply(get_word2vec_embeddings)\ndf_test['answer_embeddings'] = df_test['answer_tokens'].apply(get_word2vec_embeddings)\ndf_test['summary_embeddings'] = df_test['summary_tokens'].apply(get_word2vec_embeddings)\n\ndf_dev['question_embeddings'] = df_dev['question_tokens'].apply(get_word2vec_embeddings)\ndf_dev['answer_embeddings'] = df_dev['answer_tokens'].apply(get_word2vec_embeddings)\ndf_dev['summary_embeddings'] = df_dev['summary_tokens'].apply(get_word2vec_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GloVe embeddings","metadata":{}},{"cell_type":"code","source":"!pip install glove-python3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glove import Glove\nfrom glove import Corpus\nfrom nltk.tokenize import word_tokenize\n\nmy_embedding_size = 5\n# Tokenize the sentences\ndf_train['question_tokens'] = df_train['question'].apply(word_tokenize)\ndf_train['answer_tokens'] = df_train['answer'].apply(word_tokenize)\ndf_train['summary_tokens'] = df_train['summary'].apply(word_tokenize)\n\n# Combine all tokens for training GloVe\nall_tokens = df_train['question_tokens'].tolist() + df_train['answer_tokens'].tolist() + df_train['summary_tokens'].tolist()\n\n# Create a GloVe Corpus\ncorpus = Corpus()\ncorpus.fit(all_tokens, window=10)  # Adjusting the window size\n\n# Train the GloVe model\nglove = Glove(no_components=my_embedding_size, learning_rate=0.05)  # Setting the desired size of embeddings and learning rate\nglove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)  # Adjusting the number of epochs and threads\n\n# Adding the words and their embeddings to the model\nglove.add_dictionary(corpus.dictionary)\n\n# Function to get GloVe embeddings\ndef get_glove_embeddings(tokens):\n    embeddings = [glove.word_vectors[glove.dictionary[word]] for word in tokens if word in glove.dictionary]\n    if not embeddings:\n        # If no word has embeddings, return zeros or handle as needed\n        return [0.0] * my_embedding_size\n    return embeddings\n\n# Get GloVe embeddings for questions, answers, and summaries\ndf_train['question_embeddings'] = df_train['question_tokens'].apply(get_glove_embeddings)\ndf_train['answer_embeddings'] = df_train['answer_tokens'].apply(get_glove_embeddings)\ndf_train['summary_embeddings'] = df_train['summary_tokens'].apply(get_glove_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dev['question_tokens'] = df_dev['question'].apply(word_tokenize)\ndf_dev['answer_tokens'] = df_dev['answer'].apply(word_tokenize)\ndf_dev['summary_tokens'] = df_dev['summary'].apply(word_tokenize)\n\ndf_dev['question_embeddings'] = df_dev['question_tokens'].apply(get_glove_embeddings)\ndf_dev['answer_embeddings'] = df_dev['answer_tokens'].apply(get_glove_embeddings)\ndf_dev['summary_embeddings'] = df_dev['summary_tokens'].apply(get_glove_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Similarity scores or Distance","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import euclidean\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial.distance import cityblock\n\n\ndef calculate_distance(row):\n    # Function to calculate distance between 2 indices\n    qa_correlation = euclidean(row['question_embeddings'], row['answer_embeddings'])\n    as_correlation = euclidean(row['answer_embeddings'], row['summary_embeddings'])\n    return np.mean([ as_correlation, qa_correlation])\n\ndf_train['similarity'] = df_train.apply(calculate_distance, axis=1)\ndf_dev['similarity'] = df_dev.apply(calculate_distance, axis=1)\ndf_test['similarity'] = df_test.apply(calculate_distance, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_cosine_similarity(embeddings1, embeddings2):\n    # Convert the lists of arrays to numpy arrays\n    array1 = np.array(embeddings1)\n    array2 = np.array(embeddings2)\n    # Calculate cosine similarity\n    if array1.ndim == 1:  # Check if array1 is 1D\n        array1 = array1.reshape(1, -1)\n    if array2.ndim == 1:  # Check if array2 is 1D\n        array2 = array2.reshape(1, -1)\n    similarity_matrix = cosine_similarity(array1, array2)\n\n    # Take the mean of the cosine similarity values\n    mean_similarity = np.mean(similarity_matrix)\n\n    return mean_similarity\n\n# Apply the function to calculate cosine similarity for each row\ndf_train['question_answer_similarity'] = df_train.apply(lambda row: calculate_cosine_similarity(row['question_embeddings'], row['answer_embeddings']), axis=1)\ndf_train['answer_summary_similarity'] = df_train.apply(lambda row: calculate_cosine_similarity(row['answer_embeddings'], row['summary_embeddings']), axis=1)\ndf_train['mean_similarity'] = df_train[['question_answer_similarity', 'answer_summary_similarity']].mean(axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# May need a reshape based on type of embeddings\ndf_dev['question_answer_similarity'] = df_dev.apply(lambda row: calculate_cosine_similarity(row['question_embeddings'], row['answer_embeddings']), axis=1)\ndf_dev['answer_summary_similarity'] = df_dev.apply(lambda row: calculate_cosine_similarity(row['answer_embeddings'], row['summary_embeddings']), axis=1)\ndf_dev['mean_similarity'] = df_dev[['question_answer_similarity', 'answer_summary_similarity']].mean(axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['question_answer_similarity'] = df_test.apply(lambda row: calculate_cosine_similarity(row['question_embeddings'], row['answer_embeddings']), axis=1)\ndf_test['answer_summary_similarity'] = df_test.apply(lambda row: calculate_cosine_similarity(row['answer_embeddings'], row['summary_embeddings']), axis=1)\ndf_test['mean_similarity'] = df_test[['question_answer_similarity', 'answer_summary_similarity']].mean(axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Assigning Predictions","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\ndef assign_predictions(df):\n    # Create a new column 'predictions' initialized with 0\n    df['predictions'] = 0\n\n    # Iterate over unique 'question_embeddings'\n    for idx, row in tqdm(df.iterrows()):\n        # Find the rows with the same 'question_embeddings'\n        same_question_rows = df[df['question_embeddings'].apply(lambda x: np.array_equal(x, row['question_embeddings']))]\n\n        # Find the index of the row with the minimum 'distance'\n        min_distance_index = same_question_rows['similarity'].idxmin()         # For Distance\n        # max_distance_index = same_question_rows['mean_similarity'].idxmax()       # Uncomment this line for Cosine similarity\n\n        # Update the 'predictions' column for the row with the minimum distance\n        df.loc[min_distance_index, 'predictions'] = 1                               # For Distance\n        # df.loc[max_distance_index, 'predictions'] = 1                             # Uncomment this line for cosine similarity\n    return df\n\n# Apply predictions\ndf_dev = assign_predictions(df_dev)\ndf_train = assign_predictions(df_train)\ndf_test = assign_predictions(df_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to replace predictions that are most likely to be wrongly predicted (Cosine similarity based)\ndef assign_new_predictions(df):\n    # Create a new column 'predictions' initialized with 0\n    df['predictions'] = 0\n\n    # Iterate over unique 'question_embeddings'\n    for idx, row in tqdm(df.iterrows()):\n        # Find the rows with the same 'question_embeddings'\n        same_question_rows = df[df['question_embeddings'].apply(lambda x: np.array_equal(x, row['question_embeddings']))]\n\n        if not same_question_rows.empty:\n            # Check if there is a wrongly classified index\n            second_largest_similarity_index = same_question_rows['mean_similarity'].nlargest(2).index[1] if len(same_question_rows) > 1 else same_question_rows.index[0]\n            max_similarity_index = same_question_rows['mean_similarity'].idxax()\n            if np.abs(df['mean_similarity'][max_similarity_index] - df['mean_similarity'][second_largest_similarity_index]) <=0.0005:\n                # Update the 'predictions' column for the entire set with the second largest distance\n                df.loc[same_question_rows.index, 'predictions'] = 0\n                df.loc[second_largest_similarity_index, 'predictions'] = 1\n            else:\n                # Update the 'predictions' column for the entire set with the largest distance\n                df.loc[same_question_rows.index, 'predictions'] = 0\n                df.loc[max_similarity_index, 'predictions'] = 1\n\n            if second_largest_similarity_index==max_similarity_index:\n                df.loc[same_question_rows.index, 'predictions'] = 0\n    return df\n\ndf_dev = assign_new_predictions(df_dev)\ndf_train = assign_new_predictions(df_train)\ndf_test = assign_new_predictions(df_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis","metadata":{}},{"cell_type":"code","source":"# Function to assign Q, S, R, W\ndef assign_QSRW(df):\n  df['higher_score'] = 'Q'\n  df['R/w'] = 'R'\n  for idx, row in df.iterrows():\n      if row['question_answer_similarity'] >= row['answer_summary_similarity']:\n          df.loc[idx, 'higher_score'] = 'Q'\n      else:\n          df.loc[idx, 'higher_score'] = 'S'\n\n      if row['predictions'] == row['label']:\n        df.loc[idx, 'R/W'] = 'R'\n      else:\n        df.loc[idx, 'R/W'] = 'W'\n  return df\n\ndf_dev = assign_QSRW(df_dev)\ndf_train = assign_QSRW(df_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to check distribution of Right (R) and Wrong (W) predictions\n# Q indicates rows where Question-Answer similarity >= Summary-Answer similarity\n# S indicates rows where Summary-Answer similarity >= Question-Answer similarity\n\ndef count_combinations(df):\n    return df.groupby(['higher_score', 'R/W']).size().reset_index(name='count')\n\n# Call the function on your DataFrames\ndf_train_counts = count_combinations(df_train)\ndf_dev_counts = count_combinations(df_dev)\n\n# Display the counts\nprint(\"Training Set Counts:\")\nprint(df_train_counts)\n\nprint(\"\\nDevelopment Set Counts:\")\nprint(df_dev_counts)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report\n\n# Calculate macro F1 score\nmacro_f1 = f1_score(df_dev['label'], df_dev['predictions'], average='macro')\n\n# Print the macro F1 score\nprint(\"Macro F1 Score:\", macro_f1)\n\n# Print the detailed classification report (precision, recall, F1 score for each class)\nclassification_rep = classification_report(df_dev['label'], df_dev['predictions'])\nprint(\"Classification Report for dev set:\\n\", classification_rep)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report\n\n# Calculate macro F1 score\nmacro_f1 = f1_score(df_train['label'], df_train['predictions'], average='macro')\n\n# Print the macro F1 score\nprint(\"Macro F1 Score:\", macro_f1)\n\n# Print the detailed classification report (precision, recall, F1 score for each class)\nclassification_rep = classification_report(df_train['label'], df_train['predictions'])\nprint(\"Classification Report for train set:\\n\", classification_rep)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=df_test['predictions']","metadata":{},"execution_count":null,"outputs":[]}]}