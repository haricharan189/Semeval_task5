{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\nfrom transformers import TrainingArguments\nimport torch\nimport sys\nimport os\nfrom datasets import Dataset, DatasetDict\nfrom transformers import Trainer\nimport numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\nimport pandas as pd\nimport csv\nimport time\nimport torch.nn.functional as F\nimport argparse\nfrom transformers import EarlyStoppingCallback\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Lambda, LSTM, Dropout, BatchNormalization, Attention, Input\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Summary","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/content/original_train.csv\")    # original training set from organizers\ntest_df = pd.read_csv(\"/content/original_test.csv\")      # original test set from organizers\ndev_df = pd.read_csv(\"/content/original_dev.csv\")        # original dev set from organizers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_long_text_summary(long_text, max_length_per_section):\n    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to('cuda')\n\n    # Split the text into smaller sections\n    sections = [long_text[i:i + max_length_per_section] for i in range(0, len(long_text), max_length_per_section)]\n\n    summaries = []\n\n    for section in sections:\n        input_text = \"summarize: \" + section\n        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1000, truncation=True, padding=True)\n\n        # Adjust max_length and length_penalty as needed\n        summary_ids = model.generate(inputs.to('cuda'), max_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)\n\n        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n        summaries.append(summary)\n\n    # Concatenate the summaries for each section\n    final_summary = \" \".join(summaries)\n    return final_summary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_double_summary(df):\n    # Iterate through each row and generate summaries\n    for index, row in df.iterrows():\n        try:\n          input_text = row['explanation']\n\n          # Generate the first summary\n          summary = generate_long_text_summary(input_text, 1000)\n\n          # Use the first summary as input for the second summary\n          input_text = summary\n          summary_new = generate_long_text_summary(input_text, 300)\n\n          # Store the final summary in the 'summary' column\n          df.at[index, 'summary'] = summary_new\n        except:\n          df.at[index, 'summary'] = \" \"\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate 2nd level summary on all 3 sets\ndf_train = generate_double_summary(train_df)\ndf_test = generate_double_summary(test_df)\ndf_dev = generate_double_summary(dev_df)\n\n# Save all 3 dataframes\ndf_train.to_csv(\"/content/summary_train.csv\")\ndf_test.to_csv(\"/content/summary_test.csv\")\ndf_dev.to_csv(\"/content/summary_dev.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dev = pd.read_csv(\"/content/summary_dev.csv\") # Summarized Dev set\ndf_train = pd.read_csv(\"/content/summary_train.csv\") # Summarized Train set\ndf_test = pd.read_csv(\"/content/summary_test.csv\") # Summarized Test set","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get Legal-Bert Embeddings","metadata":{}},{"cell_type":"code","source":"# get sentence embeddings using Legal-BERT\ndef get_embeddings(sentence):\n    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n    inputs = inputs.to('cuda')\n    outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n    return embeddings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings for questions,answers and summaries of train\ndf_train['question_embeddings'] = df_train['question'].apply(get_embeddings)\ndf_train['answer_embeddings'] = df_train['answer'].apply(get_embeddings)\ndf_train['summary_embeddings'] = df_train['summary'].apply(get_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings for questions,answers and summaries of dev\ndf_dev['question_embeddings'] = df_dev['question'].apply(get_embeddings)\ndf_dev['answer_embeddings'] = df_dev['answer'].apply(get_embeddings)\ndf_dev['summary_embeddings'] = df_dev['summary'].apply(get_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings for questions,answers and summaries of test\ndf_test['question_embeddings'] = df_test['question'].apply(get_embeddings)\ndf_test['answer_embeddings'] = df_test['answer'].apply(get_embeddings)\ndf_test['summary_embeddings'] = df_test['summary'].apply(get_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the train embeddings to list\nqe_train=df_train['question_embeddings'].tolist()\nae_train= df_train['answer_embeddings'].tolist()\nse_train= df_train['summary_embeddings'].tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the dev embeddings to list\nqe_dev=df_dev['question_embeddings'].tolist()\nae_dev= df_dev['answer_embeddings'].tolist()\nse_dev= df_dev['summary_embeddings'].tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the test embeddings to list\nqe_test=df_test['question_embeddings'].tolist()\nae_test= df_test['answer_embeddings'].tolist()\nse_test= df_test['summary_embeddings'].tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use this code to convert the lists to pytorch tensors. \n#Same code applies for dev and test and is ignored here to avoid redundancy\nqe_tensor_train= torch.tensor(qe_train).to_dense()\nae_tensor_train= torch.tensor(ae_train).to_dense()\nse_tensor_train= torch.tensor(se_train).to_dense()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN feature extraction","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CNNModel(nn.Module):\n    def __init__(self, input_size, embedding_size):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=100, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n        self.dense = nn.Linear(100, embedding_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(2)  \n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.global_max_pooling(x).squeeze(-1)\n        x = self.dense(x)\n        return x\n\ndef get_cnn(input_tensor, model):\n    # Forward pass to obtain output embeddings\n    embeddings = model(input_tensor)\n    return embeddings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#question cnn features\nq_cnn_train= get_cnn(qe_tensor_train)\nq_cnn_dev = get_cnn(qe_tensor_dev)\nq_cnn_test = get_cnn(qe_tensor_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#answer cnn features\na_cnn_train= get_cnn(ae_tensor_train)\na_cnn_dev = get_cnn(ae_tensor_dev)\na_cnn_test = get_cnn(ae_tensor_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#summary cnn features\ns_cnn_train= get_cnn(se_tensor_train)\ns_cnn_dev = get_cnn(se_tensor_dev)\ns_cnn_test = get_cnn(se_tensor_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi-level CNN Feature Fusion Approach","metadata":{}},{"cell_type":"code","source":"# Define the CNN model\nclass CNNModel(nn.Module):\n    def __init__(self, input_size, embedding_size):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=100, kernel_size=3,padding=1)\n        self.relu = nn.ReLU()\n        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n        self.dense = nn.Linear(100, embedding_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(2)  # Add a dummy dimension for the channel\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.global_max_pooling(x).squeeze(-1)\n        x = self.dense(x)\n        return x\n\nnum_examples = 666\nbert_embedding_size = 768\n# Create an instance of the model\nmodel = CNNModel(input_size=bert_embedding_size, embedding_size=100)\n# Forward pass to obtain output embeddings\nq_cnn_train_fus = model(qe_tensor_train)\nprint(\"Output Embeddings Shape:\", q_cnn_train_fus.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self, input_size, embedding_size):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=100, kernel_size=4,padding=2)\n        self.relu = nn.ReLU()\n        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n        self.dense = nn.Linear(100, embedding_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(2)  # Add a dummy dimension for the channel\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.global_max_pooling(x).squeeze(-1)\n        x = self.dense(x)\n        return x\n\nnum_examples = 666\nbert_embedding_size1 = 100\n\n# Create an instance of the model\nmodel1 = CNNModel(input_size=bert_embedding_size1, embedding_size=100)\n# Forward pass to obtain output embeddings. The previous output is passed as input to this new CNN layer.\nq_cnn_train_fus1 = model1(q_cnn_train_fus)\nprint(\"Output Embeddings Shape:\", q_cnn_train_fus1.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self, input_size, embedding_size):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=100, kernel_size=5,padding=3)\n        self.relu = nn.ReLU()\n        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n        self.dense = nn.Linear(100, embedding_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(2)  # Add a dummy dimension for the channel\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.global_max_pooling(x).squeeze(-1)\n        x = self.dense(x)\n        return x\n\nnum_examples = 666\nbert_embedding_size2 = 100\n\n# Create an instance of the model\nmodel2 = CNNModel(input_size=bert_embedding_size2, embedding_size=100)\n# Forward pass to obtain output embeddings. The previous output is passed as input to this new layer.\nq_cnn_train_fus2 = model2(q_cnn_train_fus1)\nprint(\"Output Embeddings Shape:\", q_cnn_train_fus2.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The above multi-level fusion code can be used to obtain embeddings for answer, summary of dev and test as well.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GRU","metadata":{}},{"cell_type":"code","source":"def get_bi_gru(input_tensor):\n    # Define the Bi-GRU layer\n    bi_gru = nn.GRU(input_size=768, hidden_size=100, bidirectional=True)\n\n    # Pass the input tensor through the Bi-GRU layer\n    gru_output, _ = bi_gru(input_tensor.unsqueeze(0))\n\n    # Extract the hidden state for each timestep\n    hidden_states = gru_output.view(input_tensor.size(0), 100, 2)#input_tensor= 666 for train,84 for dev and 98 for test\n\n    # Take the average of the hidden states from both directions\n    avg_embeddings = torch.mean(hidden_states, dim=2)\n\n    return avg_embeddings\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Question gru features\nq_gru_train= get_bi_gru(qe_tensor_train)\nq_gru_dev = get_bi_gru(qe_tensor_dev)\nq_gru_test = get_bi_gru(qe_tensor_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#answer gru features\na_gru_train= get_bi_gru(ae_tensor_train)\na_gru_dev = get_bi_gru(ae_tensor_dev)\na_gru_test = get_bi_gru(ae_tensor_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#summary gru features\ns_gru_train= get_bi_gru(se_tensor_train)\ns_gru_dev = get_bi_gru(se_tensor_dev)\ns_gru_test = get_bi_gru(se_tensor_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"def get_lstm(input_tensor):\n    # Define the Bi-LSTM layer\n    bi_lstm = nn.LSTM(input_size=768, hidden_size=100, bidirectional=True)\n\n    # Pass the input tensor through the Bi-LSTM layer\n    lstm_output, _ = bi_lstm(input_tensor.unsqueeze(0))\n\n    # Extract the hidden state for each timestep\n    hidden_states = lstm_output.view(input_tensor.size(0), 100, 2)\n\n    # Optionally, take the average of the hidden states from both directions\n    avg_embeddings = torch.mean(hidden_states, dim=2)\n\n    return avg_embeddings\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Question LSTM features\nq_lstm_train= get_lstm(qe_tensor_train)\nq_lstm_dev = get_lstm(qe_tensor_dev)\nq_lstm_test = get_lstm(qe_tensor_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#answer LSTM features\na_lstm_train= get_lstm(ae_tensor_train)\na_lstm_dev = get_lstm(ae_tensor_dev)\na_lstm_test = get_lstm(ae_tensor_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#summary LSTM features\ns_lstm_train= get_lstm(se_tensor_train)\ns_lstm_dev = get_lstm(se_tensor_dev)\ns_lstm_test = get_lstm(se_tensor_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONCAT","metadata":{}},{"cell_type":"code","source":"c_q_train = torch.cat((q_cnn_train, q_gru_train, q_lstm_train),dim=1)\nc_a_train= torch.cat((a_cnn_train,a_gru_train,a_lstm_train,dim=1)\nc_s_train= torch.cat((s_cnn_train,s_gru_train,s_lstm_train,dim=1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_q_dev = torch.cat((q_cnn_dev, q_gru_dev, q_lstm_dev),dim=1)\nc_a_dev= torch.cat((a_cnn_dev,a_gru_dev,a_lstm_dev,dim=1)\nc_s_dev= torch.cat((s_cnn_dev,s_gru_dev,s_lstm_dev,dim=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_q_test = torch.cat((q_cnn_test, q_gru_test, q_lstm_test),dim=1)\nc_a_test= torch.cat((a_cnn_test,a_gru_test,a_lstm_test,dim=1)\nc_s_test= torch.cat((s_cnn_test,s_gru_test,s_lstm_test,dim=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtaining the labels from train and dev set.\nlabel_list_train= df_train['label'].to_list()\nlabel_tensor_train= torch.tensor(label_list_train)\nlabel_list_dev= df_dev['label'].to_list()\nlabel_tensor_dev= torch.tensor(label_list_dev)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(c_q_train.detach().numpy())\ndf2= pd.DataFrame(c_a_train.detach().numpy())\ndf3= pd.DataFrame(c_s_train.detach().numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4 = pd.DataFrame(c_q_dev.detach().numpy())\ndf5= pd.DataFrame(c_a_dev.detach().numpy())\ndf6= pd.DataFrame(c_s_dev.detach().numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df7 = pd.DataFrame(c_q_test.detach().numpy())\ndf8= pd.DataFrame(c_a_test.detach().numpy())\ndf9= pd.DataFrame(c_s_test.detach().numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concating into a single dataframe\nresult_df_train = pd.concat([df1, df2,df3,df_train['label']], axis=1)\nresult_df_dev = pd.concat([df4, df5,df6,df_dev['label']], axis=1)\nresult_df_test = pd.concat([df7, df8,df9], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df_train.columns = result_df_train.columns.astype(str)\nresult_df_dev.columns = result_df_dev.columns.astype(str)\nresult_df_test.columns = result_df_test.columns.astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obtaining the csv files\nresult_df_train.to_csv('summary_train.csv',index=False)\nresult_df_dev.to_csv('summary_dev.csv',index=False)\nresult_df_test.to_csv('summary_test.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MULTI-LEVEL CONCAT APPROACH","metadata":{}},{"cell_type":"code","source":"mf_train_q_1= torch.cat((q_cnn_train_fus, q_cnn_train_fus1), dim=1)# Output of first layer concated with that of second layer.\nmf_train_q_2= torch.cat((mf_train_q_1,q_cnn_train_fus2),dim=1)# The previous concated embedding is further concated with the second layer's output.\nfinal_q_train= torch.cat((mf_train_q_2,q_lstm_train,q_gru_train),dim=1) # The multi-level cnn is futher concated with lstm and gru.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The above code can be used to obtained multi-level features of answers and summaries of dev and train as well and is ignored here to avoid redundancy.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1D CNN Model","metadata":{}},{"cell_type":"code","source":"# Define the CNN model\nmodel = Sequential()\n\n# Convolutional layer with ReLU activation\nmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(900, 1)))\n\n# Max pooling layer\nmodel.add(MaxPooling1D(pool_size=2))\n\n# Flatten layer to convert 2D output to 1D\nmodel.add(Flatten())\n\n# Fully connected (dense) layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer with linear activation\nmodel.add(Dense(1, activation='linear'))\n\n# Custom activation layer with learnable threshold\nmodel.add(Lambda(lambda x: K.sigmoid(x - K.mean(x)), output_shape=(1,)))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"#defining training data\nX_train= result_df_train.drop('label',axis=1) \ny_train= result_df_train['label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting the model\nmodel.fit(X_train, y_train, epochs=15, batch_size=32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defining dev data\nX_dev= result_df_dev.drop('label',axis=1)\ny_dev= result_df_dev['label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obtaining predictions\ny_pred_dev= model.predict(X_dev)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MANUAL GRID SEARCH FOR THRESHOLD","metadata":{}},{"cell_type":"code","source":"# defining an array of explorable thresholds and initializing the best threshold to 0.\nthresholds = np.arange(0.01, 1.0, 0.01)\nbest_threshold = 0\nbest_macro_f1 = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#searching through all the thresholds and obtaining the best one with its corresponding f1-score\nfor threshold in thresholds:\n    y_predi = (y_pred > threshold).astype(int)\n    current_macro_f1 = f1_score(y_test, y_predi, average='macro')\n\n    print(f\"Threshold: {threshold:.2f}, Macro F1: {current_macro_f1:.4f}\")\n\n    if current_macro_f1 > best_macro_f1:\n        best_macro_f1 = current_macro_f1\n        best_threshold = threshold\n\nprint(f\"Best Threshold: {best_threshold}\")\nprint(f\"Best Macro F1 Score: {best_macro_f1}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obtaining the final binary prediction using the best threshold\ny_final_dev = (y_pred_dev >= best_threshold ).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obtaining classification report on dev data.\nreport_dev = classification_report(y_final_dev,y_dev)\nprint(\"Classification Report:\\n\", report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = result_df_test\ny_pred_test= model.predict(X_test)\ny_final_test = (y_pred_test >= best_threshold ).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}