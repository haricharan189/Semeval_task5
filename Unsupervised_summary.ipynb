{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "mgexctub3HNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSq58pM371fi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-CMqoWY7ae0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Summary"
      ],
      "metadata": {
        "id": "89UfLar8jrUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/original_train.csv\")    # original training set from organizers\n",
        "test_df = pd.read_csv(\"/content/original_test.csv\")      # original test set from organizers\n",
        "dev_df = pd.read_csv(\"/content/original_dev.csv\")        # original dev set from organizers"
      ],
      "metadata": {
        "id": "iKi-N8f9ktJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_long_text_summary(long_text, max_length_per_section):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to('cuda')\n",
        "\n",
        "    # Split the text into smaller sections\n",
        "    sections = [long_text[i:i + max_length_per_section] for i in range(0, len(long_text), max_length_per_section)]\n",
        "\n",
        "    summaries = []\n",
        "\n",
        "    for section in sections:\n",
        "        input_text = \"summarize: \" + section\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1000, truncation=True, padding=True)\n",
        "\n",
        "        # Adjust max_length and length_penalty as needed\n",
        "        summary_ids = model.generate(inputs.to('cuda'), max_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Concatenate the summaries for each section\n",
        "    final_summary = \" \".join(summaries)\n",
        "    return final_summary"
      ],
      "metadata": {
        "id": "IwLhkVgKjua9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_double_summary(df):\n",
        "    # Iterate through each row and generate summaries\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "          input_text = row['explanation']\n",
        "\n",
        "          # Generate the first summary\n",
        "          summary = generate_long_text_summary(input_text, 1000)\n",
        "\n",
        "          # Use the first summary as input for the second summary\n",
        "          input_text = summary\n",
        "          summary_new = generate_long_text_summary(input_text, 300)\n",
        "\n",
        "          # Store the final summary in the 'summary' column\n",
        "          df.at[index, 'summary'] = summary_new\n",
        "        except:\n",
        "          df.at[index, 'summary'] = \" \"\n",
        "    return df"
      ],
      "metadata": {
        "id": "JKWZMNLhj3bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 2nd level summary on all 3 sets\n",
        "df_train = generate_double_summary(train_df)\n",
        "df_test = generate_double_summary(test_df)\n",
        "df_dev = generate_double_summary(dev_df)\n",
        "\n",
        "# Save all 3 dataframes\n",
        "df_train.to_csv(\"/content/summary_train.csv\")\n",
        "df_test.to_csv(\"/content/summary_test.csv\")\n",
        "df_dev.to_csv(\"/content/summary_dev.csv\")"
      ],
      "metadata": {
        "id": "o9rSdMkjlxmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TrQzf0G77R3"
      },
      "outputs": [],
      "source": [
        "df_dev = pd.read_csv(\"/content/summary_dev.csv\") # Summarized Dev set\n",
        "df_train = pd.read_csv(\"/content/summary_train.csv\") # Summarized Train set\n",
        "df_test = pd.read_csv(\"/content/summary_test.csv\") # Summarized Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAdSJTXUmMXU"
      },
      "source": [
        "# Transformer embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfgXpv758nKx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('LambdaX-AI/legal-deberta-v1')\n",
        "deberta_model = AutoModel.from_pretrained('LambdaX-AI/legal-deberta-v1').to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuXofa2d9bIw"
      },
      "outputs": [],
      "source": [
        "# Function to get sentence embeddings using Transformer\n",
        "def get_embeddings(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs = inputs.to('cuda')\n",
        "    outputs = deberta_model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSeq4A949-Bz"
      },
      "outputs": [],
      "source": [
        "# Get embeddings for questions and answers for training set\n",
        "df_train['question_embeddings'] = df_train['question'].apply(get_embeddings)\n",
        "df_train['answer_embeddings'] = df_train['answer'].apply(get_embeddings)\n",
        "df_train['summary_embeddings'] = df_train['summary'].apply(get_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrbFwajjWEd"
      },
      "outputs": [],
      "source": [
        "# Get embeddings for questions and answers for dev set\n",
        "df_dev['question_embeddings'] = df_dev['question'].apply(get_embeddings)\n",
        "df_dev['answer_embeddings'] = df_dev['answer'].apply(get_embeddings)\n",
        "df_dev['summary_embeddings'] = df_dev['summary'].apply(get_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings for questions and answers for test set\n",
        "df_test['question_embeddings'] = df_test['question'].apply(get_embeddings)\n",
        "df_test['answer_embeddings'] = df_test['answer'].apply(get_embeddings)\n",
        "df_test['summary_embeddings'] = df_test['summary'].apply(get_embeddings)"
      ],
      "metadata": {
        "id": "tFQNMRSmh8OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28--Tu7omQ_w"
      },
      "source": [
        "# Word2Vec embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0VDBLP8UMjP"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Tokenize the sentences\n",
        "df_train['question_tokens'] = df_train['question'].apply(word_tokenize)\n",
        "df_train['answer_tokens'] = df_train['answer'].apply(word_tokenize)\n",
        "df_train['summary_tokens'] = df_train['summary'].apply(word_tokenize)\n",
        "\n",
        "df_dev['question_tokens'] = df_dev['question'].apply(word_tokenize)\n",
        "df_dev['answer_tokens'] = df_dev['answer'].apply(word_tokenize)\n",
        "df_dev['summary_tokens'] = df_dev['summary'].apply(word_tokenize)\n",
        "\n",
        "df_test['question_tokens'] = df_test['question'].apply(word_tokenize)\n",
        "df_test['answer_tokens'] = df_test['answer'].apply(word_tokenize)\n",
        "df_test['summary_tokens'] = df_test['summary'].apply(word_tokenize)\n",
        "\n",
        "my_embedding_size = 5\n",
        "# Train a Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=df_train['question_tokens'].tolist() +\n",
        "                                   df_train['answer_tokens'].tolist() +\n",
        "                                   df_train['summary_tokens'].tolist() +\n",
        "                                   df_dev['question_tokens'].tolist() +\n",
        "                                   df_dev['answer_tokens'].tolist() +\n",
        "                                   df_dev['summary_tokens'].tolist() +\n",
        "                                   df_test['question_tokens'].tolist() +\n",
        "                                   df_test['answer_tokens'].tolist() +\n",
        "                                   df_test['summary_tokens'].tolist(),\n",
        "                          vector_size=my_embedding_size,  # Setting the desired size of embeddings\n",
        "                          window=7,  # Adjusting the window size\n",
        "                          min_count=1,  # Minimum word frequency to be included in the model\n",
        "                          workers=2)  # Number of CPU cores to use during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9MKnbZdVDjg"
      },
      "outputs": [],
      "source": [
        "def get_word2vec_embeddings(tokens):\n",
        "    embeddings = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
        "    if not embeddings:\n",
        "        # If no word has embeddings, return zeros or handle as needed\n",
        "        return [0.0] * your_embedding_size\n",
        "    return embeddings\n",
        "\n",
        "# Get Word2Vec embeddings for questions, answers, and summaries\n",
        "df_train['question_embeddings'] = df_train['question_tokens'].apply(get_word2vec_embeddings)\n",
        "df_train['answer_embeddings'] = df_train['answer_tokens'].apply(get_word2vec_embeddings)\n",
        "df_train['summary_embeddings'] = df_train['summary_tokens'].apply(get_word2vec_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2B4Zl7ZVIwt"
      },
      "outputs": [],
      "source": [
        "# Get Word2Vec embeddings for questions, answers, and summaries for test and dev set\n",
        "df_test['question_embeddings'] = df_test['question_tokens'].apply(get_word2vec_embeddings)\n",
        "df_test['answer_embeddings'] = df_test['answer_tokens'].apply(get_word2vec_embeddings)\n",
        "df_test['summary_embeddings'] = df_test['summary_tokens'].apply(get_word2vec_embeddings)\n",
        "\n",
        "df_dev['question_embeddings'] = df_dev['question_tokens'].apply(get_word2vec_embeddings)\n",
        "df_dev['answer_embeddings'] = df_dev['answer_tokens'].apply(get_word2vec_embeddings)\n",
        "df_dev['summary_embeddings'] = df_dev['summary_tokens'].apply(get_word2vec_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz1ay4RenNbU"
      },
      "source": [
        "# GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-xmLD0enXUt"
      },
      "outputs": [],
      "source": [
        "!pip install glove-python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0CG_VP0nSXW"
      },
      "outputs": [],
      "source": [
        "from glove import Glove\n",
        "from glove import Corpus\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "your_embedding_size = 5\n",
        "# Tokenize the sentences\n",
        "df_train['question_tokens'] = df_train['question'].apply(word_tokenize)\n",
        "df_train['answer_tokens'] = df_train['answer'].apply(word_tokenize)\n",
        "df_train['summary_tokens'] = df_train['summary'].apply(word_tokenize)\n",
        "\n",
        "# Combine all tokens for training GloVe\n",
        "all_tokens = df_train['question_tokens'].tolist() + df_train['answer_tokens'].tolist() + df_train['summary_tokens'].tolist()\n",
        "\n",
        "# Create a GloVe Corpus\n",
        "corpus = Corpus()\n",
        "corpus.fit(all_tokens, window=10)  # Adjusting the window size\n",
        "\n",
        "# Train the GloVe model\n",
        "glove = Glove(no_components=your_embedding_size, learning_rate=0.05)  # Setting the desired size of embeddings and learning rate\n",
        "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)  # Adjusting the number of epochs and threads\n",
        "\n",
        "# Adding the words and their embeddings to the model\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "\n",
        "# Function to get GloVe embeddings\n",
        "def get_glove_embeddings(tokens):\n",
        "    embeddings = [glove.word_vectors[glove.dictionary[word]] for word in tokens if word in glove.dictionary]\n",
        "    if not embeddings:\n",
        "        # If no word has embeddings, return zeros or handle as needed\n",
        "        return [0.0] * your_embedding_size\n",
        "    return embeddings\n",
        "\n",
        "# Get GloVe embeddings for questions, answers, and summaries\n",
        "df_train['question_embeddings'] = df_train['question_tokens'].apply(get_glove_embeddings)\n",
        "df_train['answer_embeddings'] = df_train['answer_tokens'].apply(get_glove_embeddings)\n",
        "df_train['summary_embeddings'] = df_train['summary_tokens'].apply(get_glove_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKmF8SdUo44l"
      },
      "outputs": [],
      "source": [
        "df_dev['question_tokens'] = df_dev['question'].apply(word_tokenize)\n",
        "df_dev['answer_tokens'] = df_dev['answer'].apply(word_tokenize)\n",
        "df_dev['summary_tokens'] = df_dev['summary'].apply(word_tokenize)\n",
        "\n",
        "df_dev['question_embeddings'] = df_dev['question_tokens'].apply(get_glove_embeddings)\n",
        "df_dev['answer_embeddings'] = df_dev['answer_tokens'].apply(get_glove_embeddings)\n",
        "df_dev['summary_embeddings'] = df_dev['summary_tokens'].apply(get_glove_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity scores or Distance"
      ],
      "metadata": {
        "id": "XaRJmvSOnvU9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3duYn55-9Td"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "\n",
        "def calculate_distance(row):\n",
        "    # Function to calculate distance between 2 indices\n",
        "    qa_correlation = cityblock(row['question_embeddings'], row['answer_embeddings'])\n",
        "    as_correlation = cityblock(row['answer_embeddings'], row['summary_embeddings'])\n",
        "    return np.mean([ as_correlation, qa_correlation])\n",
        "\n",
        "df_train['similarity'] = df_train.apply(calculate_distance, axis=1)\n",
        "df_dev['similarity'] = df_dev.apply(calculate_distance, axis=1)\n",
        "df_test['similarity'] = df_test.apply(calculate_distance, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJakWWbHhGdM"
      },
      "outputs": [],
      "source": [
        "def calculate_cosine_similarity(embeddings1, embeddings2):\n",
        "    # Convert the lists of arrays to numpy arrays\n",
        "    array1 = np.array(embeddings1)\n",
        "    array2 = np.array(embeddings2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_matrix = cosine_similarity(array1.reshape(1, -1), array2.reshape(1, -1))\n",
        "\n",
        "    # Take the mean of the cosine similarity values\n",
        "    mean_similarity = np.mean(similarity_matrix)\n",
        "\n",
        "    return mean_similarity\n",
        "\n",
        "# Apply the function to calculate cosine similarity for each row\n",
        "df_train['question_answer_similarity'] = df_train.apply(lambda row: calculate_cosine_similarity(row['question_embeddings'], row['answer_embeddings']), axis=1)\n",
        "df_train['answer_summary_similarity'] = df_train.apply(lambda row: calculate_cosine_similarity(row['answer_embeddings'], row['summary_embeddings']), axis=1)\n",
        "df_train['mean_similarity'] = df_train[['question_answer_similarity', 'answer_summary_similarity']].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOnXiM1VVmUW"
      },
      "outputs": [],
      "source": [
        "# May need a reshape based on type of embeddings\n",
        "df_dev['question_answer_similarity'] = df_dev.apply(lambda row: calculate_cosine_similarity(row['question_embeddings'], row['answer_embeddings']), axis=1)\n",
        "df_dev['answer_summary_similarity'] = df_dev.apply(lambda row: calculate_cosine_similarity(row['answer_embeddings'], row['summary_embeddings']), axis=1)\n",
        "df_dev['mean_similarity'] = df_dev[['question_answer_similarity', 'answer_summary_similarity']].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['question_answer_similarity'] = df_test.apply(lambda row: calculate_cosine_similarity(row['question_embeddings'], row['answer_embeddings']), axis=1)\n",
        "df_test['answer_summary_similarity'] = df_test.apply(lambda row: calculate_cosine_similarity(row['answer_embeddings'], row['summary_embeddings']), axis=1)\n",
        "df_test['mean_similarity'] = df_test[['question_answer_similarity', 'answer_summary_similarity']].mean(axis=1)"
      ],
      "metadata": {
        "id": "48XAYBuzlhYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_embedding(df):\n",
        "    # Function to reshape embeddings when explanaton is not available\n",
        "    # Check if the embedding is 1D and reshape if needed\n",
        "    for idx, row in df.iterrows():\n",
        "      embedding = df['summary_embeddings'][idx]\n",
        "      embedding = np.array(embedding)\n",
        "      if len(embedding.shape) == 1:\n",
        "        embedding = embedding.reshape(1, -1)\n",
        "        print(idx)\n",
        "        df.at[idx, 'summary_embeddings'] = embedding\n",
        "        x = df['summary_embeddings'][idx]\n",
        "        print(x.shape)\n",
        "    return df\n",
        "\n",
        "df_test = reshape_embedding(df_test)"
      ],
      "metadata": {
        "id": "XO3XkGvpRnX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assigning Predictions"
      ],
      "metadata": {
        "id": "faemr5xk2NGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8kJV5kBDsmt"
      },
      "outputs": [],
      "source": [
        "def assign_predictions(df):\n",
        "    # Create a new column 'predictions' initialized with 0\n",
        "    df['predictions'] = 0\n",
        "\n",
        "    # Iterate over unique 'question_embeddings'\n",
        "    for idx, row in df.iterrows():\n",
        "        # Find the rows with the same 'question_embeddings'\n",
        "        same_question_rows = df[df['question_embeddings'].apply(lambda x: np.array_equal(x, row['question_embeddings']))]\n",
        "\n",
        "        # Find the index of the row with the minimum 'distance'\n",
        "        min_distance_index = same_question_rows['mean_similarity'].idxmin()         # For Distance\n",
        "        # max_distance_index = same_question_rows['mean_similarity'].idxmax()       # Uncomment this line for Cosine similarity\n",
        "\n",
        "        # Update the 'predictions' column for the row with the minimum distance\n",
        "        df.loc[min_distance_index, 'predictions'] = 1                               # For Distance\n",
        "        # df.loc[max_distance_index, 'predictions'] = 1                             # Uncomment this line for cosine similarity\n",
        "    return df\n",
        "\n",
        "# Apply predictions\n",
        "df_dev = assign_predictions(df_dev)\n",
        "df_train = assign_predictions(df_train)\n",
        "df_test = assign_predictions(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to replace predictions that are most likely to be wromgly predicted (Cosine similarity based)\n",
        "\n",
        "def assign_new_predictions(df):\n",
        "    # Create a new column 'predictions' initialized with 0\n",
        "    df['predictions'] = 0\n",
        "\n",
        "    # Iterate over unique 'question_embeddings'\n",
        "    for idx, row in df.iterrows():\n",
        "        # Find the rows with the same 'question_embeddings'\n",
        "        same_question_rows = df[df['question_embeddings'].apply(lambda x: np.array_equal(x, row['question_embeddings']))]\n",
        "\n",
        "        if not same_question_rows.empty:\n",
        "            # Check if there is a wrongly classified index\n",
        "            second_largest_similarity_index = same_question_rows['mean_similarity'].nlargest(2).index[1] if len(same_question_rows) > 1 else same_question_rows.index[0]\n",
        "            max_similarity_index = same_question_rows['mean_similarity'].idxmax()\n",
        "            if df['mean_similarity'][max_similarity_index] - df['mean_similarity'][second_largest_similarity_index] <=0.0005:\n",
        "                # Update the 'predictions' column for the entire set with the second largest distance\n",
        "                df.loc[same_question_rows.index, 'predictions'] = 0\n",
        "                df.loc[second_largest_similarity_index, 'predictions'] = 1\n",
        "            else:\n",
        "                # Update the 'predictions' column for the entire set with the largest distance\n",
        "                df.loc[same_question_rows.index, 'predictions'] = 0\n",
        "                df.loc[max_similarity_index, 'predictions'] = 1\n",
        "\n",
        "            if second_largest_distance_index==max_similarity_index:\n",
        "                df.loc[same_question_rows.index, 'predictions'] = 0\n",
        "    return df\n",
        "\n",
        "df_dev = assign_new_predictions(df_dev)\n",
        "df_train = assign_new_predictions(df_train)\n",
        "df_test = assign_new_predictions(df_test)"
      ],
      "metadata": {
        "id": "ZKws3-OlAiIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "DR-uB5wx1_FG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxo8pMDYrqSD"
      },
      "outputs": [],
      "source": [
        "# Function to check distribution of Right (R) and Wrong (W) predictions\n",
        "# Q indicates rows where Question-Answer similarity >= Summary-Answer similarity\n",
        "# S indicates rows where Summary-Answer similarity >= Question-Answer similarity\n",
        "\n",
        "def count_combinations(df):\n",
        "    return df.groupby(['higher_score', 'R/W']).size().reset_index(name='count')\n",
        "\n",
        "# Call the function on your DataFrames\n",
        "df_train_counts = count_combinations(df_train)\n",
        "df_dev_counts = count_combinations(df_dev)\n",
        "\n",
        "# Display the counts\n",
        "print(\"Training Set Counts:\")\n",
        "print(df_train_counts)\n",
        "\n",
        "print(\"\\nDevelopment Set Counts:\")\n",
        "print(df_dev_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G3rEy1zHQL4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Calculate macro F1 score\n",
        "macro_f1 = f1_score(df_dev['label'], df_dev['predictions'], average='macro')\n",
        "\n",
        "# Print the macro F1 score\n",
        "print(\"Macro F1 Score:\", macro_f1)\n",
        "\n",
        "# Print the detailed classification report (precision, recall, F1 score for each class)\n",
        "classification_rep = classification_report(df_dev['label'], df_dev['predictions'])\n",
        "print(\"Classification Report for dev set:\\n\", classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Calculate macro F1 score\n",
        "macro_f1 = f1_score(df_train['label'], df_train['predictions'], average='macro')\n",
        "\n",
        "# Print the macro F1 score\n",
        "print(\"Macro F1 Score:\", macro_f1)\n",
        "\n",
        "# Print the detailed classification report (precision, recall, F1 score for each class)\n",
        "classification_rep = classification_report(df_train['label'], df_train['predictions'])\n",
        "print(\"Classification Report for train set:\\n\", classification_rep)"
      ],
      "metadata": {
        "id": "fUpqYwlWWkW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=df_test['predictions']"
      ],
      "metadata": {
        "id": "dKi0LrDMBR1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}